ifelse(df$student == "Yes", 1, 0)
)
# Frequencies of `default`
table(df$default)
# Set seed
set.seed(4321)
# Obtain training and testing indices
train_index <- sample(
1:nrow(df), round(nrow(df) * 0.80)
)
test_index <- setdiff(1:nrow(df), train_index)
# Split data
train_df <- df[train_index,]
test_df <- df[test_index,]
# Training frequencies
table(train_df$default)
# Testing frequencies
table(test_df$default)
# Fit logistic regression model
logm_train <- glm(
default ~ .,
data = train_df,
family = "binomial"
)
# Print summary
summary(logm_train)
# Fit logistic regression model
logm_train <- glm(
default ~ .,
data = train_df[,-4],
family = "binomial"
)
# Print summary
summary(logm_train)
# Obtain predicted probabilities
predicted_probs <- predict(
logm_train,
newdata = test_df,
type = "response"
)
# Obtain predicted classes
predicted_classes <- ifelse(
predicted_probs > 0.50, 1, 0
)
# Obtain confusion matrix
confusion_matrix <- table(
factor(predicted_classes),
factor(test_df$default)
)
# Print confusion matrix
confusion_matrix
# Compute model accuracy
sum(diag(confusion_matrix)) /
sum(confusion_matrix)
# `diag` will obtain the diagonal of a matrix
# in our confusion matrix, the diagonal corresponds
# to when our model correctly predicted "No" default
# and when our model correctly predicted "Yes" default
# Taking the sum of the diagonal gives us the total
# correct predictions
# Taking the sum of the confusion matrix gives us the
# total number of possible observations
# Obtain confusion matrix
confusionMatrix(
data = factor(predicted_classes),
reference = factor(test_df$default),
positive = "1"
)
# Training control
train_parameters <- trainControl(
method = "cv", # cross-validation
number = 10 # 10 folds
)
# Perform cross-validation
train_cv <- train(
default ~ .,
data = df,
method = "glm",
family = "binomial",
trControl = train_parameters
)
# Load packages
library(tidyverse); library(caret)
# Read in data
df <- read.csv("Desktop/DS3100 TA/lab/lab_w9/Default.csv")
# Create dummy variable
df$default <- factor(
ifelse(df$default == "Yes", 1, 0)
)
# Create dummy variable (and factor it)
df$student <- factor(
ifelse(df$student == "Yes", 1, 0)
)
# Frequencies of `default`
table(df$default)
# Set seed
set.seed(4321)
# Obtain training and testing indices
train_index <- sample(
1:nrow(df), round(nrow(df) * 0.80)
)
test_index <- setdiff(1:nrow(df), train_index)
# Split data
train_df <- df[train_index,]
test_df <- df[test_index,]
# Training frequencies
table(train_df$default)
# Testing frequencies
table(test_df$default)
# Fit logistic regression model
logm_train <- glm(
default ~ .,
data = train_df,
family = "binomial"
)
# Print summary
summary(logm_train)
# Fit logistic regression model
logm_train <- glm(
default ~ .,
data = train_df[,-4],
family = "binomial"
)
# Print summary
summary(logm_train)
# Obtain predicted probabilities
predicted_probs <- predict(
logm_train,
newdata = test_df,
type = "response"
)
# Obtain predicted classes
predicted_classes <- ifelse(
predicted_probs > 0.50, 1, 0
)
# Obtain confusion matrix
confusion_matrix <- table(
factor(predicted_classes),
factor(test_df$default)
)
# Print confusion matrix
confusion_matrix
# Compute model accuracy
sum(diag(confusion_matrix)) /
sum(confusion_matrix)
# `diag` will obtain the diagonal of a matrix
# in our confusion matrix, the diagonal corresponds
# to when our model correctly predicted "No" default
# and when our model correctly predicted "Yes" default
# Taking the sum of the diagonal gives us the total
# correct predictions
# Taking the sum of the confusion matrix gives us the
# total number of possible observations
# Obtain confusion matrix
confusionMatrix(
data = factor(predicted_classes),
reference = factor(test_df$default),
positive = "1"
)
# Training control
train_parameters <- trainControl(
method = "cv", # cross-validation
number = 10 # 10 folds
)
# Perform cross-validation
train_cv <- train(
default ~ .,
data = df,
method = "glm",
family = "binomial",
trControl = train_parameters
)
# Load packages
library(tidyverse); library(caret)
# Read in data
df <- read.csv("Desktop/DS3100 TA/lab/lab_w9/Default.csv")
# Create dummy variable
df$default <- factor(
ifelse(df$default == "Yes", 1, 0)
)
# Create dummy variable (and factor it)
df$student <- factor(
ifelse(df$student == "Yes", 1, 0)
)
# Frequencies of `default`
table(df$default)
# Set seed
set.seed(4321)
# Obtain training and testing indices
train_index <- sample(
1:nrow(df), round(nrow(df) * 0.80)
)
test_index <- setdiff(1:nrow(df), train_index)
# Split data
train_df <- df[train_index,]
test_df <- df[test_index,]
# Training frequencies
table(train_df$default)
# Testing frequencies
table(test_df$default)
# Fit logistic regression model
logm_train <- glm(
default ~ .,
data = train_df,
family = "binomial"
)
# Print summary
summary(logm_train)
# Fit logistic regression model
logm_train <- glm(
default ~ .,
data = train_df[,-4],
family = "binomial"
)
# Print summary
summary(logm_train)
# Obtain predicted probabilities
predicted_probs <- predict(
logm_train,
newdata = test_df,
type = "response"
)
# Obtain predicted classes
predicted_classes <- ifelse(
predicted_probs > 0.50, 1, 0
)
# Obtain confusion matrix
confusion_matrix <- table(
factor(predicted_classes),
factor(test_df$default)
)
# Print confusion matrix
confusion_matrix
# Compute model accuracy
sum(diag(confusion_matrix)) /
sum(confusion_matrix)
# `diag` will obtain the diagonal of a matrix
# in our confusion matrix, the diagonal corresponds
# to when our model correctly predicted "No" default
# and when our model correctly predicted "Yes" default
# Taking the sum of the diagonal gives us the total
# correct predictions
# Taking the sum of the confusion matrix gives us the
# total number of possible observations
# Obtain confusion matrix
confusionMatrix(
data = factor(predicted_classes),
reference = factor(test_df$default),
positive = "1"
)
# Training control
train_parameters <- trainControl(
method = "cv", # cross-validation
number = 10 # 10 folds
)
# Perform cross-validation
train_cv <- train(
default ~ .,
data = df,
method = "glm",
family = "binomial",
trControl = train_parameters
)
install.packages("FactoMineR")
# Read in data
degrees <- read.csv("degrees-that-pay-back.csv")
# Read in data
degrees <- read.csv("degrees-that-pay-back.csv")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
# Read in data
degrees <- read.csv("degrees-that-pay-back.csv")
# Read in data
degrees <- read.csv("Desktop/DS3100 TA/lab/lab_w11/degrees-that-pay-back.csv")
# Load packages
library(tidyverse); library(cluster);
library(factoextra); library(NbClust);
library(aricode); library(ggplot2);
# Rename rows
row.names(degrees) <- degrees$Undergraduate.Major
# Obtain variables of interest
degrees_interest <- degrees %>%
select(
GPT.4.Category, Starting.Median.Salary,
Mid.Career.Median.Salary,
Percent.change.from.Starting.to.Mid.Career.Salary,
Mid.Career.10th.Percentile.Salary,
Mid.Career.90th.Percentile.Salary
)
# Obtain variables of interest
table(degrees_interest$GPT.4.Category)
# Remove "Education & Training" and "Natural Resources"
degrees_final <- degrees_interest %>%
filter(
GPT.4.Category != "Education & Training",
GPT.4.Category != "Natural Resources"
)
# Obtain variables of interest
table(degrees_final$GPT.4.Category)
# Create long data frame
degrees_long <- degrees_final %>%
pivot_longer(
cols = c(
"Starting.Median.Salary",
"Mid.Career.Median.Salary"
),
names_to = "Salary",
values_to = "US Dollars"
)
# Rename values
degrees_long$Salary <- ifelse(
degrees_long$Salary == "Starting.Median.Salary",
"Starting Median", "Mid-Career Median"
)
# Create faceted plot
ggplot(
data = degrees_long,
aes(
x = Salary,
y = `US Dollars`
)
) +
geom_boxplot(width = 0.5) +
facet_wrap(~ GPT.4.Category) +
labs(x = "Salary") +
scale_y_continuous(
limits = c(0, 125000),
breaks = seq(0, 125000, 25000),
expand = c(0, 0)
) +
theme(
panel.background = element_blank(),
axis.line = element_line(linewidth = 1),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Create numeric categories
numeric_categories <- as.numeric(as.factor(degrees_final$GPT.4.Category))
numeric_categories
# Set a seed
set.seed(1234)
# Remove GPT-4's categories
numeric_salaries <- degrees_final %>%
select(-GPT.4.Category)
# Perform k-means with 7 clusters
theoretical_kmeans <- kmeans(
x = numeric_salaries, # numeric data
centers = 7, # number of clusters
iter.max = 10, # number of maximum iterations
nstart = 25 # number of random starting values
)
# Within-cluster sum of squares
theoretical_kmeans$withinss
# Variance explained
theoretical_kmeans$betweenss / # between sum of squares
theoretical_kmeans$totss # total sum of squares
# Cluster frequencies
table(theoretical_kmeans$cluster)
# Cluster centroids
round(theoretical_kmeans$centers)
# Visualize K-means
fviz_cluster(
object = theoretical_kmeans, # K-means output
data = numeric_salaries # numeric data
)
# Set seed
set.seed(1234)
# Remove "Physician Assistant" observation
numeric_categories <- numeric_categories[
-which(row.names(numeric_salaries) == "Physician Assistant")
]
numeric_salaries <- numeric_salaries[
-which(row.names(numeric_salaries) == "Physician Assistant"),
]
# Perform k-means with 6 clusters
theoretical_kmeans <- kmeans(
x = numeric_salaries, # numeric data
centers = 6, # number of clusters
iter.max = 10, # number of maximum iterations
nstart = 25 # number of random starting values
)
# Visualize K-means
fviz_cluster(
object = theoretical_kmeans, # K-means output
data = numeric_salaries # numeric data
)
# Variance explained
theoretical_kmeans$betweenss / # between sum of squares
theoretical_kmeans$totss # total sum of squares
# Adjusted Rand Index
ARI(numeric_categories, theoretical_kmeans$cluster)
# Adjusted Mutual Information
AMI(numeric_categories, theoretical_kmeans$cluster)
numeric_categories
# Create numeric categories
numeric_categories <- as.numeric(as.factor(degrees_final$GPT.4.Category))
numeric_categories
numeric_salaries <- degrees_final %>%
select(-GPT.4.Category)
numeric_salaries
install.packages("tidyverse")
install.packages("nycflights13")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(nycflights13)
library(tidyverse)
flights <- flights
flights
# State the obvious~
# How many variables do we have in the tibble?
# How many observations do we have in the tibble?
( jan1 = filter(flights, month == 1, day == 1) )
install.packages("tidyverse")
install.packages("nycflights13")
library(nycflights13)
library(tidyverse)
flights <- flights
flights
( jan1 = filter(flights, month == 1, day == 1) )
( jan1 = filter(flights, month == 1, day == 1 | day == 2) )
jan1
jan2
( jan2 = filter(flights, month == 1, day == 1 & day == 2) )
jan2
jan2
library(nycflights13)
library(tidyverse)
( further_delay_0 = filter(flights, dep_delay > 0 & arr_delay-dep_delay > 0))
( make_up_0 = filter(flights, dep_delay > 0 & arr_delay-dep_delay < 0))
jan_1 <- 1
jan_1
jan_1 <- filter(flights,
month == 1,
day == 1
)
jan_1
july_21 <- filter(flights,
month == 7,
day == 21
)
july_21
july_21 <- filter(data = flights,
month == 7,
day == 21
)
df = read.csv('../R_data_analysis.csv')
ls
ls
df = read.csv('../R_data_analysis.csv')
getwd()
setwd('Desktop')
getwd()
setwd("Alvin's Folder/MS Courses/Healthcare")
getwd()
setwd("R-for-Med-EntryMLDL/Visualization_Statistical_Analysis in R")
getwd()
df = read.csv('../R_data_analysis.csv')
getwd()
getwd()
df = read.csv("/Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/R_data_analysis.csv")
df = read.csv("/Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/R_data_analysis.csv")
library(tidyverse)
library(ggplot2)
df = read.csv("/Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/R_data_analysis.csv")
df
df_subset <- df[c(1, 3), c(2, 3)]
df_subset
df_subset <- df[, c(2, 3)]
df_subset
df
df_subset
df_subset <- df[, c(, 8)]
df_subset <- df[, c(0, 8)]
df_subset
df_subset
df
df_subset <- df[, c(0, 7)]
df_subset
df_subset <- df[, c(1, 7)]
df_subset
df_subset <- df[, c(0,1,2,3,4,5,6,7)]
df_subset
df_subset <- df[, 1:8]
df_subset
clean_df <- replace(df_subset, is.na(df), 0)
clean_df <- replace(df_subset, is.na(df_subset), 0)
clean_df
getwd()
write.csv(clean_df, file = "fdata.csv", row.names = FALSE)
getwd()
write.csv(clean_df, file = "Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/Visualization_Statistical_Analysis_in_R/fdata.csv", row.names = FALSE)
getwd()
write.csv(clean_df, file = "Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/fdata.csv", row.names = FALSE)
getwd()
write.csv(clean_df, file = "Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/fdata.csv", row.names = FALSE)
write.csv(clean_df, file = "fdata.csv", row.names = FALSE)
write.csv(clean_df, file = "fdata.csv",quote = F, row.names = FALSE)
View(clean_df)
View(clean_df)
getwd()
View(clean_df)
write.csv(clean_df, file = "fdata.csv")
write.csv(clean_df, file = "/Users/alvin.nebula23/Desktop/Alvin's Folder/MS Courses/Healthcare/R-for-Med-EntryMLDL/Visualization_Statistical_Analysis_in_R/fdata.csv")
